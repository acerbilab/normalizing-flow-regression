[AdvancedOptions]
# Explicit noise handling (0: none; 1: unknown noise level; 2: user-provided noise)
uncertainty_handling = []

## Flow configurations
# Type of transform to use between linear layers. choices=["affine-coupling""quadratic-coupling","rq-coupling","affine-autoregressive","quadratic-autoregressive","rq-autoregressive"].
base_transform_type = "affine-autoregressive"
# choices=["reverse", "permutation", "lu", "svd"]
linear_transform_type = "reverse"
# Number of hidden features to use in coupling/autoregressive nets.
hidden_features = 32
# Box is on [-bound, bound]^2 for spline flow.
tail_bound = 3
# Number of bins to use for piecewise transforms for spline flow.
num_bins = 8
# Number of blocks to use in coupling/autoregressive nets.
num_transform_blocks = 2
# Whether to use batch norm in coupling/autoregressive nets.
use_batch_norm = False
# Dropout probability for coupling/autoregressive nets.
dropout_probability = 0.0
# Whether to unconditionally transform 'identity' features in coupling layer.
apply_unconditional_transform = True
# Activation function
activation_fn = "relu"
# Number of intermidiate transform layers
n_layers = 9
# Use different lnZ estimates for the transform layers
seperate_lnZ = True
# Constrain the MAF transformation scales(alpha) and shifts(mu) ranges
constrain_transform_ranges = True
# Range scales of the transformation parameters
inscale_alpha = 1.0
outcale_alpha = 1.5
inscale_mu = 1.0
outscale_mu = 1.0

## Flow training options
# Penalization coefficient for weights
lambd_weight = 12.5
# Flow optimizer
flow_optimizer = "lbfgs"
# L-BFGS max iterations
lbfgs_max_iter = 500
# L-BFGS max function evaluations
lbfgs_max_eval = 2000
# LBFGS stopping criteria settings
# L-BFGS tolerance change for directional derivative
lbfgs_tolerance_change = 1e-5
# L-BFGS length of running loss history:
lbfgs_tolerance_history_length = 5
# L-BFGS absolute tolerance for loss change (1e-5 is usually strict and 1e-3 is loose)
lbfgs_abs_tol = 1e-3

## Additional flow optimization settings
# How many MLE iterations
mle_max_iter = 500
# Whether to optimzie lnZ before optimizing flow:
optimize_lnZ = True
# Whether to set bounds on lnZ
constrain_lnZ = False

## Optimization options
# Number of warmup steps
num_warmup = 20
# Total number of iterations for NFR
nfr_iter = 30

## Low-density noise options
# Discount observations from from extremely low-density regions
noise_shaping = True
# Threshold from max observed value to start discounting
noise_shaping_threshold = 10 * D
# Proportionality factor of added noise wrt distance from threshold
noise_shaping_factor = 0.05
# Minimum added noise
noise_shaping_min = 0.0
# Added noise at threshold
noise_shaping_med = 0.0
# Maximum ratio of max/min added noise
noise_shaping_max_ratio = np.inf

# Input transform for bounded variables
bounded_transform = "probit"
# Evaluated log likelihood values at X0
log_likes = []
# Evaluated log prior values at X0
log_priors_orig = []
# Evaluated std values at X0
S_orig = []

# Size of cache for storing fcn evaluations
cache_size = 500
# Initial samples (plausible is uniform in the plausible box)
init_design = "provided"
# Minimum observation noise
tol_noise = np.sqrt(1e-5)
# Min number of iterations
min_iter = 30

# Number of flow ensembles
n_ensembles = 1
# Whether to use annealed target posterior as warm-up
annealed_target = True
# Annealing schedule for target posterior ["linear", "quadratic"]
target_annealing_schedule = "linear"

## p0 initialization
# Weight top 50% points by the order of log densities
init_p0_reweight = False
# p0 as the diagonal Gaussian if it's estimated from top 50% points
init_p0_diag = True
# p0 as the standard normal
init_p0_as_standard_normal = False

# How many surrogate's samples to save at each iteration.
save_surrogate_samples = 0
# Loss type, one of ["MSE", "Tobit", "TobitGaussian"]
loss_type = "Tobit"
# A scale parameter for Tobit likelihood's std
eta_thresh = 0.05
# difference from the max density value (actually a lower confidence bound if noisy) for declaring the low-density region
low_region_delta_thresh = 50 * D
# Below this threshold, we return to pure Tobit likelihood if (loss_type == "TobitGaussian")
super_low_region_delta_thresh = 20 * 20 * D
# Cache initial samples only
generate_cache_only = False

# Rotate and scale input
warp_rotoscaling = True
# Regularization weight towards diagonal covariance matrix for N training inputs
warp_cov_reg = 0
# Threshold True correlation matrix for roto-scaling
warp_roto_corr_thresh = 0.05
# Use top 50% points for estimating p0
p0_top_proportion = 0.5
# Use threshold for selecting points for p0
init_p0_threshold = True
# Threshold value for selecting points for p0
init_p0_threshold_value = 10 * D

# Call wandb.log every wandb_log_steps steps
wandb_log_steps = 100
# Turn off the transform to unbounded space
turn_off_unbounded_transform = False
# for loss_type = "NormalPDF", use truncated normal distribution
truncate_normal_pdf = False

fit_initial_flow_method = "base_dist"
